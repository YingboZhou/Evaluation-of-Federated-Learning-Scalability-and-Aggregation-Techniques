{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9331e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from subprocess import call\n",
    "import requests \n",
    "np.random.seed(0)\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf5753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for loading and downloading the dataset\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "\tdataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values\n",
    " \n",
    "# load a list of files, such as x, y, z data for a given variable\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = np.dstack(loaded)\n",
    "\treturn loaded\n",
    " \n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset(group, prefix=''):\n",
    "\tfilepath = prefix + group + '/Inertial Signals/'\n",
    "\tfilenames = list()\n",
    "\t# body acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y\n",
    "\n",
    "# Framing data by windows\n",
    "def segmentData(accData,time_step,step):\n",
    "    segmentAccData = list()\n",
    "    for i in range(0, accData.shape[0] - time_step,step):\n",
    "        segmentAccData.append(accData[i:i+time_step,:])\n",
    "    return segmentAccData\n",
    "\n",
    "# download function for datasets\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2777fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and unzipping dataset\n",
    "os.makedirs('dataset',exist_ok=True)\n",
    "print(\"downloading...\")            \n",
    "data_directory = os.path.abspath(\"dataset/UCI HAR Dataset.zip\")\n",
    "if not os.path.exists(data_directory):\n",
    "    download_url(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\",data_directory)\n",
    "    print(\"download done\")\n",
    "else:\n",
    "    print(\"dataset already downloaded\")\n",
    "    \n",
    "data_directory2 = os.path.abspath(\"dataset/UCI HAR Dataset\")\n",
    "if not os.path.exists(data_directory2): \n",
    "    print(\"extracting data\")\n",
    "    with zipfile.ZipFile(data_directory, 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.abspath(\"dataset/\"))\n",
    "    print(\"data extracted in \" + data_directory2)\n",
    "else:\n",
    "    print(\"Data already extracted in \" + data_directory2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69739fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all train\n",
    "trainX, trainy = load_dataset('train', 'dataset/UCI HAR Dataset/')\n",
    "trainy = np.asarray([x - 1 for x in trainy])\n",
    "\n",
    "# load all test\n",
    "testX, testy = load_dataset('test', 'dataset/UCI HAR Dataset/')\n",
    "testy = np.asarray([x - 1 for x in testy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78950a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining datasets\n",
    "datasets = list()\n",
    "for x in range(0,trainX.shape[2]):\n",
    "    datasets.append(np.concatenate((trainX[:,:,x],testX[:,:,x]), axis = 0))\n",
    "datasets = np.dstack(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing and normalizing dataset\n",
    "\n",
    "meanAcc = np.mean(datasets[:,:,:3])\n",
    "stdAcc = np.std(datasets[:,:,:3])\n",
    "varAcc = np.var(datasets[:,:,:3])\n",
    "stackedFeaturesAcc = np.hstack((meanAcc,stdAcc,varAcc))\n",
    "\n",
    "meanGyro = np.mean(datasets[:,:,3:])\n",
    "stdGyro = np.std(datasets[:,:,3:])\n",
    "varGyro = np.var(datasets[:,:,3:])\n",
    "stackedFeaturesGyro = np.hstack((meanGyro,stdGyro,varGyro))\n",
    "\n",
    "normalizedTrainAcc = (trainX[:,:,:3] - meanAcc) / stdAcc\n",
    "normalizedTrainGyro = (trainX[:,:,3:] - meanGyro) / stdGyro\n",
    "\n",
    "normalizedTestAcc = (testX[:,:,:3] - meanAcc) / stdAcc\n",
    "normalizedTestGyro = (testX[:,:,3:] - meanGyro) / stdGyro\n",
    "\n",
    "normalizedAllAcc = (datasets[:,:,:3] - meanAcc) / stdAcc\n",
    "normalizedAllGyro = (datasets[:,:,3:] - meanGyro) / stdGyro\n",
    "\n",
    "stackedFeatures = np.vstack((stackedFeaturesAcc,stackedFeaturesGyro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d47785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking datasets\n",
    "normalizedX = np.dstack((normalizedTrainAcc,normalizedTrainGyro))\n",
    "normalizedEval = np.dstack((normalizedTestAcc,normalizedTestGyro))\n",
    "normalizedAll = np.dstack((normalizedAllAcc,normalizedAllGyro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the UCI dataset\n",
    "dataName = 'UCI'\n",
    "os.makedirs('datasetStandardized/'+dataName+ '/train', exist_ok=True)\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/train/AccX'+dataName+'.csv', normalizedX[:,:,0], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/train/AccY'+dataName+'.csv', normalizedX[:,:,1], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/train/AccZ'+dataName+'.csv', normalizedX[:,:,2], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/train/GyroX'+dataName+'.csv', normalizedX[:,:,3], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/train/GyroY'+dataName+'.csv', normalizedX[:,:,4], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/train/GyroZ'+dataName+'.csv', normalizedX[:,:,5], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/train/Label'+dataName+'.csv', trainy, delimiter=',')\n",
    "\n",
    "os.makedirs('datasetStandardized/'+dataName+ '/eval', exist_ok=True)\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/eval/AccX'+dataName+'.csv', normalizedEval[:,:,0], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/eval/AccY'+dataName+'.csv', normalizedEval[:,:,1], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/eval/AccZ'+dataName+'.csv', normalizedEval[:,:,2], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/eval/GyroX'+dataName+'.csv', normalizedEval[:,:,3], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/eval/GyroY'+dataName+'.csv', normalizedEval[:,:,4], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/eval/GyroZ'+dataName+'.csv', normalizedEval[:,:,5], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/eval/Label'+dataName+'.csv', testy, delimiter=',')\n",
    "\n",
    "os.makedirs('datasetStandardized/'+dataName+ '/all', exist_ok=True)\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/all/AccX'+dataName+'.csv', normalizedAll[:,:,0], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/all/AccY'+dataName+'.csv', normalizedAll[:,:,1], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/all/AccZ'+dataName+'.csv', normalizedAll[:,:,2], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/all/GyroX'+dataName+'.csv', normalizedAll[:,:,3], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/all/GyroY'+dataName+'.csv', normalizedAll[:,:,4], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/all/GyroZ'+dataName+'.csv', normalizedAll[:,:,5], delimiter=',')\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/all/Label'+dataName+'.csv', testy, delimiter=',')\n",
    "\n",
    "os.makedirs('datasetStandardized/'+dataName+ '/features', exist_ok=True)\n",
    "np.savetxt('datasetStandardized/'+dataName+ '/features/mean-std-var'+dataName+'.csv', stackedFeatures, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
